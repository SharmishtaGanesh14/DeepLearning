PANDAS DATAFRAME CHEAT-SHEET (Concise, assessment-ready)
=======================================================
1) Imports & aliases
--------------------
import pandas as pd
import numpy as np
2) DataFrame creation
---------------------
# From list/dict/array
df = pd.DataFrame([[1,'A'],[2,'B']], columns=['id','label'])
df = pd.DataFrame({'gene':['G1','G2'], 'expr':[0.5,2.3]})
arr = np.array([[1,2,3],[4,5,6]])
df = pd.DataFrame(arr, columns=['c1','c2','c3'])
3) Reading / Writing Files (CSV, TSV, Excel, Parquet, HDF5, SQL)
----------------------------------------------------------------
# CSV / TSV
df = pd.read_csv('data.csv')
df_tsv = pd.read_csv('data.tsv', sep='\t')
df.to_csv('out.csv', index=False)
# Excel
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df.to_excel('out.xlsx', index=False', sheet_name='sheet1')
# Parquet / Feather
df.to_parquet('data.parquet')
df = pd.read_parquet('data.parquet')
# HDF5
df.to_hdf('data.h5', 'table', mode='w')
df = pd.read_hdf('data.h5', 'table')
# SQL (example using SQLAlchemy)
from sqlalchemy import create_engine
engine = create_engine('sqlite:///mydb.sqlite')
df.to_sql('table_name', engine, if_exists='replace', index=False)
df_sql = pd.read_sql('SELECT * FROM table_name', engine)
4) File I/O (plain files), JSON & Pickling
------------------------------------------
# Plain text / binary file writing and reading
with open('notes.txt', 'w', encoding='utf-8') as f:
f.write('example\\n')
with open('notes.txt', 'r', encoding='utf-8') as f:
txt = f.read()
# JSON (structured)
import json
d = {'sample':'S1', 'value': 12}
with open('meta.json','w') as f:
json.dump(d, f)
with open('meta.json','r') as f:
d2 = json.load(f)
# Pickling Python objects
import pickle
obj = {'a':1, 'b':[1,2,3]}
with open('obj.pkl','wb') as f:
pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
with open('obj.pkl','rb') as f:
obj2 = pickle.load(f)
# Pandas-specific pickling (fast for DataFrame)
df.to_pickle('df.pkl')
df2 = pd.read_pickle('df.pkl')
# joblib (alternative for large arrays/objects)
# from joblib import dump, load
# dump(obj, 'obj.joblib')
# obj = load('obj.joblib')
5) Viewing / Inspecting
-----------------------
df.head(5)
df.tail(3)
df.info()
df.describe()
df.shape
df.dtypes
df.memory_usage(deep=True)
6) Column & Row Selection
-------------------------
# Columns
df['gene'] # Series
df[['gene','expr']] # DataFrame
# .loc (label), .iloc (integer)
df.loc[0, 'gene']
df.iloc[0:5, 0:2]
# Boolean filtering
df[df['expr'] > 1.0]
df[(df['expr']>1.0) & df['gene'].str.contains('^G')]
# .query (string expression, engine='python' for python funcs)
df.query('expr > 1.0 and gene.str.startswith("G")', engine='python')
# Assignment (avoid chained indexing)
df.loc[df['expr']>1, 'flag'] = 'high'
7) Add / Remove Columns or Rows
-------------------------------
df['log_expr'] = np.log1p(df['expr'])
df = df.assign(expr2 = df['expr']*2)
df.insert(1, 'id', [101,102])
df.drop(columns=['id'], inplace=False)
df.drop(index=[0,1])
# Concatenate rows/columns
df_concat = pd.concat([df1, df2], axis=0, ignore_index=True)
8) Rename & Reorder
-------------------
df.rename(columns={'expr':'expression'}, inplace=True)
df.rename(index={0:'sample1'}, inplace=True)
cols = ['Gene','expression','log_expr']
df = df[cols]
9) Grouping & Aggregation
-------------------------
df.groupby('condition')['expression'].mean()
df.groupby('condition').agg(mean_expr=('expression','mean'),
sum_expr=('expression','sum'),
count=('expression','count')).reset_index()
df.groupby(['cond','time']).agg({'expression':['mean','std']})
10) Sorting
-----------
df.sort_values(by='expression', ascending=False)
df.sort_index()
11) Missing Values
------------------
df.isnull().sum()
df.dropna(axis=0, how='any')
df.dropna(axis=1, thresh=5)
df.fillna(0)
df['expression'].fillna(df['expression'].mean(), inplace=True)
df.fillna(method='ffill')
df.interpolate(method='linear')
12) Merge / Join / Concatenate
------------------------------
df_concat = pd.concat([df1, df2], axis=0)
merged = pd.merge(df_left, df_right, on='sample_id', how='inner') # inner,left,right,outer
df_left.join(df_right, how='left')
df_left.combine_first(df_right)
13) Remove Duplicates
---------------------
df.drop_duplicates(subset=['gene','time'], keep='first')
14) Apply / Map / Applymap
--------------------------
df['log_expr'] = df['expression'].apply(np.log1p)
df['label2'] = df['label'].map({'A':'alpha','B':'beta'})
df[['c1','c2']] = df[['c1','c2']].applymap(lambda x: x*2)
df['flag'] = np.where(df['expression']>1.0, 'high', 'low')
15) Pivot / Reshape: pivot, pivot_table, melt, stack, unstack
--------------------------------------------------------------
df_long = df.melt(id_vars=['sample','condition'], value_vars=['geneA','geneB'],
var_name='gene', value_name='expr')
df_wide = df_long.pivot(index='sample', columns='gene', values='expr')
df_pivot = df_long.pivot_table(index='sample', columns='gene', values='expr',
aggfunc='mean', fill_value=0)
stacked = df_wide.stack()
unstacked = stacked.unstack(level=1)
16) Categorical & Datetime
--------------------------
df['tissue'] = df['tissue'].astype('category')
df['tissue'].cat.categories
df['tissue'] = df['tissue'].cat.reorder_categories(['liver','heart','brain'], ordered=True)
df['date'] = pd.to_datetime(df['date_string'], format='%Y-%m-%d')
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['weekday'] = df['date'].dt.day_name()
17) Filtering by Condition or Value
----------------------------------
df[df['gene'].isin(['G1','G3'])]
df[~df['gene'].isin(['G1','G3'])]
df[df['Gene'].str.contains('STAT', case=False, na=False)]
18) Advanced Indexing
---------------------
val = df.at[5, 'expression'] # label scalar, fast
val2 = df.iat[5, 2] # integer position scalar
df_multi = df.set_index(['gene','time'])
df_multi.xs('G1', level='gene')
19) Value counts & Crosstabs
----------------------------
df['tissue'].value_counts()
pd.crosstab(df['condition'], df['tissue'])
df['gene'].nunique()
20) Exporting
-------------
df.to_csv('cleaned.csv', index=False)
df.to_excel('out.xlsx', index=False)
df.to_parquet('out.parquet')
df.to_sql('table', engine, index=False)
21) Performance & Memory (tips)
-------------------------------
- Use vectorized ops; avoid Python loops.
- Convert low-cardinality strings to 'category'.
- Downcast numeric types: pd.to_numeric(..., downcast='integer'/'float')
- Use memory_usage(deep=True) to find heavy columns.
- Read large CSVs in chunks: pd.read_csv(..., chunksize=100000)
- Use parquet/feather for faster I/O.
- For out-of-core: Dask, Vaex.
22) Iterating rows (when unavoidable)
------------------------------------
# itertuples is faster
for row in df.itertuples(index=True, name='Row'):
pass
# iterrows is slower and returns Series
for idx, row in df.iterrows():
pass
23) Custom functions for bio/clinical wrangling
-----------------------------------------------
# Example: log2 CPM for count matrix (DataFrame: genes x samples)
def log2_cpm(counts_df):
counts = counts_df.astype(float)
libsize = counts.sum(axis=0)
cpm = counts.div(libsize, axis=1) * 1e6
return np.log2(cpm + 1)
# Example: replicate imputation rules (brief)
# - all 3 NaN -> drop gene
# - 1 of 3 NaN -> fill with mean of other two
# - 2 of 3 NaN -> fill with remaining value
24) Common utilities & best practices
------------------------------------
df['col'].unique()
df['status'].replace({'NA':'unknown'}, inplace=True)
df['expr'] = df['expr'].clip(lower=0)
df.groupby('gene')['expr'].transform(lambda x: (x-x.mean())/x.std())
df.reset_index(drop=True)
# Use .loc for assignments to avoid SettingWithCopyWarning
# Prefer returning new objects rather than using inplace=True
END OF CHEAT-SHEET



#!/usr/bin/env python3
"""
PANDAS DATAFRAME OPERATIONS CHEAT SHEET FOR ASSESSMENTS
=======================================================

This script demonstrates ALL common pandas DataFrame operations that could be asked in
a bioinformatics/data science assessment. Each section includes:

- Brief description
- Example code snippet
- Sample input/output (using small DataFrames)
- Why it's useful in bioinformatics (e.g., RNA-Seq, gene expression analysis)

Run this script to see outputs in console. Use as a reference during assessments.

Key Libraries: pandas, numpy
Assumes basic setup: import pandas as pd; import numpy as np

Sections:
1. Loading & Saving Data
2. Inspection & Exploration
3. Selection & Filtering
4. Missing Data Handling
5. Grouping & Aggregation
6. Apply/Map/Lambda Operations
7. Merging & Joining
8. Reshaping (Pivot/Melt/Stack)
9. Sorting & Ranking
10. Statistical Operations
11. String/Text Operations
12. Time Series (if applicable)
13. Advanced: MultiIndex, Window Functions

Author: Grok (for Elucidate Assessment Prep)
Date: October 30, 2025
"""

import pandas as pd
import numpy as np
from datetime import datetime

# ====================== SAMPLE DATAFRAMES ======================
# Create sample gene expression data (like RNA-Seq)
genes = ['TP53', 'MYC', 'STAT1', 'FOXA2', 'ABL1']
samples = ['S1', 'S2', 'S3', 'S4', 'S5']
times = [0, 0, 4, 4, 12]

expr_data = {
    'Gene': np.repeat(genes, len(samples)),
    'Sample': np.tile(samples, len(genes)),
    'Time': np.tile(times, len(genes)),
    'Expression': np.random.uniform(10, 100, len(genes) * len(samples)),
    'Log2FC': np.random.uniform(-2, 2, len(genes) * len(samples)),
    'PValue': np.random.uniform(0.001, 0.1, len(genes) * len(samples))
}

# Introduce some NaNs for missing data examples
expr_data['Expression'][5] = np.nan  # Missing in MYC_S2
expr_data['PValue'][10] = np.nan     # Missing in STAT1_S3

df = pd.DataFrame(expr_data)

# Metadata-like DataFrame
meta_data = {
    'Sample': samples,
    'Time': times,
    'Replicate': ['R1', 'R2', 'R1', 'R2', 'R3']
}
meta_df = pd.DataFrame(meta_data)

print("Sample DataFrame (df):")
print(df.head())
print("\nSample Metadata (meta_df):")
print(meta_df.head())
print("=" * 80)

# ====================== 1. LOADING & SAVING DATA ======================
"""
LOADING: Read CSV/Excel/TSV with options for index, headers, NaN handling.
Useful: Load gene expression matrices or metadata.
"""
# Load CSV (index_col=0 for gene names as index)
# df_loaded = pd.read_csv('gene_data.csv', index_col=0)

# Load with NaN as empty
# df_loaded = pd.read_csv('data.csv', na_values=['', 'NA', 'nan'])

# Save to CSV/Excel (no index for clean export)
# df.to_csv('cleaned.csv', index=False)  # No gene index in output
# df.to_excel('output.xlsx', sheet_name='Expression')

# Example output
print("\n1. LOADING/SAVING:")
df.to_csv('temp_sample.csv', index=False)
loaded = pd.read_csv('temp_sample.csv')
print(f"Loaded shape: {loaded.shape}")

# ====================== 2. INSPECTION & EXPLORATION ======================
"""
HEAD/TAIL: First/last rows.
INFO: Data types, non-null counts.
DESCRIBE: Stats summary.
SHAPE/COLUMNS/INDEX: Dimensions.
USEFUL: Quick data overview in RNA-Seq pipeline.
"""
print("\n2. INSPECTION:")
print("Head(3):\n", df.head(3))
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print("Index:", df.index.tolist())
print("Info:\n", df.info())
print("Describe:\n", df.describe())

# Value counts for categorical
print("Time value counts:\n", df['Time'].value_counts())

# ====================== 3. SELECTION & FILTERING ======================
"""
LOC/ILOC: Label/position-based selection.
BOOLEAN MASK: Filter rows (e.g., high expression).
QUERY: SQL-like filtering.
USEFUL: Select DE genes (PValue < 0.05), samples by time.
"""
print("\n3. SELECTION/FILTERING:")
# loc: by label
print("Genes at Time 0:\n", df.loc[df['Time'] == 0, ['Gene', 'Expression']])

# iloc: by position
print("First 3 rows, first 2 cols:\n", df.iloc[:3, :2])

# Boolean mask
high_expr = df['Expression'] > 50
print("High expression genes:\n", df[high_expr]['Gene'].unique())

# Query
de_genes = df.query('PValue < 0.05 and abs(Log2FC) > 1')
print("DE genes count:", len(de_genes))

# ====================== 4. MISSING DATA HANDLING ======================
"""
ISNA/SUM: Detect/count NaNs.
FILLNA: Impute (mean, forward-fill).
DROPNA: Remove rows/cols with NaNs.
USEFUL: Handle NaNs in RNA-Seq (impute by replicate mean).
"""
print("\n4. MISSING DATA:")
print("NaNs per column:\n", df.isna().sum())

# Fill NaN in Expression with column mean
df_filled = df.copy()
df_filled['Expression'] = df_filled['Expression'].fillna(df_filled['Expression'].mean())
print("After mean fill - NaNs:\n", df_filled.isna().sum())

# Drop rows with any NaN
df_clean = df.dropna()
print("After dropna - shape:", df_clean.shape)

# Advanced: Group-wise fill (e.g., by Time)
df['Expression_filled'] = df.groupby('Time')['Expression'].transform(lambda x: x.fillna(x.mean()))

# ====================== 5. GROUPING & AGGREGATION ======================
"""
GROUPBY: Group by column, apply agg funcs (mean, sum, count).
AGG: Multiple functions.
TRANSFORM: Broadcast results back to original shape.
USEFUL: Mean expression per time point/replicate.
"""
print("\n5. GROUPING:")
grouped = df.groupby('Time')['Expression'].agg(['mean', 'std', 'count'])
print("Mean expr by Time:\n", grouped)

# Multiple groups
multi_group = df.groupby(['Time', 'Replicate'])['Expression'].mean().unstack()
print("Mean by Time & Replicate:\n", multi_group)

# Transform (keeps shape)
df['Expr_mean_by_time'] = df.groupby('Time')['Expression'].transform('mean')
print("Transform example (first 5):\n", df[['Time', 'Expression', 'Expr_mean_by_time']].head())

# ====================== 6. APPLY/MAP/LAMBDA OPERATIONS ======================
"""
APPLY: Row/column-wise custom function.
MAP: Dict-based replacement.
LAMBDAs: Inline transformations.
USEFUL: Log-transform expression, categorize FC.
"""
print("\n6. APPLY/MAP:")
# Apply lambda to column
df['LogExpr'] = df['Expression'].apply(lambda x: np.log2(x + 1) if pd.notna(x) else np.nan)
print("Log2(Expr+1) first 5:\n", df[['Expression', 'LogExpr']].head())

# Map for categorization
fc_map = {-2: 'Down', 0: 'No Change', 2: 'Up'}
df['FC_Category'] = df['Log2FC'].map(lambda x: 'Down' if x < -1 else 'Up' if x > 1 else 'No Change')
print("FC categories:\n", df['FC_Category'].value_counts())

# Apply to rows (e.g., compute row sum)
df['Row_Sum'] = df.apply(lambda row: row['Expression'] + row['Log2FC'], axis=1)
print("Row sum example:\n", df[['Expression', 'Log2FC', 'Row_Sum']].head())

# ====================== 7. MERGING & JOINING ======================
"""
MERGE: SQL-like join on keys.
JOIN: Index-based.
CONCAT: Stack DataFrames.
USEFUL: Merge expression data with metadata/annotations.
"""
print("\n7. MERGING/JOINING:")
# Merge df with meta_df on 'Sample'
merged = pd.merge(df, meta_df, on='Sample', how='left')
print("Merged (first 5):\n", merged.head())

# Concat vertically (add rows)
df2 = df.head(3).copy()  # Fake new data
concat_v = pd.concat([df, df2], ignore_index=True)
print("Concat vertical shape:", concat_v.shape)

# Concat horizontally (add cols)
concat_h = pd.concat([df[['Gene', 'Expression']], meta_df], axis=1)
print("Concat horizontal (first 3):\n", concat_h.head(3))

# ====================== 8. RESHAPING (PIVOT/MELT/STACK) ======================
"""
PIVOT: Wide format (samples as cols).
MELT: Long format (for plotting).
STACK/UNSTACK: MultiIndex reshape.
USEFUL: Pivot genes x samples for heatmaps; melt for ggplot.
"""
print("\n8. RESHAPING:")
# Pivot: Genes x Samples
pivot_df = df.pivot(index='Gene', columns='Sample', values='Expression')
print("Pivoted (head):\n", pivot_df.head())

# Melt: From wide to long
melted = pd.melt(pivot_df.reset_index(), id_vars='Gene', var_name='Sample', value_name='Expression')
print("Melted (first 5):\n", melted.head())

# Stack/Unstack for MultiIndex
multi_idx = df.set_index(['Time', 'Gene'])['Expression'].unstack('Gene')
print("Unstacked by Gene:\n", multi_idx.head())

# ====================== 9. SORTING & RANKING ======================
"""
SORT_VALUES: By column(s).
SORT_INDEX: By index.
RANK: Assign ranks.
USEFUL: Sort DE genes by PValue; rank by fold-change.
"""
print("\n9. SORTING/RANKING:")
sorted_df = df.sort_values(['PValue', 'Expression'], ascending=[True, False])
print("Sorted by PValue asc, Expr desc (first 5):\n", sorted_df[['Gene', 'PValue', 'Expression']].head())

# Rank
df['Expr_Rank'] = df['Expression'].rank(ascending=False)
print("Expression ranks (first 5):\n", df[['Expression', 'Expr_Rank']].head())

# ====================== 10. STATISTICAL OPERATIONS ======================
"""
MEAN/MEDIAN/STD: Basic stats.
CORR/COV: Correlations.
CUMSUM/CUMPROD: Cumulative.
USEFUL: Correlation between genes; t-tests on replicates.
"""
print("\n10. STATS:")
print("Column means:\n", df[['Expression', 'Log2FC', 'PValue']].mean())

# Correlation matrix
corr_matrix = df[['Expression', 'Log2FC', 'PValue']].corr()
print("Correlation matrix:\n", corr_matrix)

# Cumulative sum by Time
df_sorted = df.sort_values('Time')
df_sorted['Cum_Expr'] = df_sorted.groupby('Gene')['Expression'].cumsum()
print("Cumulative sum (first 5):\n", df_sorted[['Gene', 'Time', 'Expression', 'Cum_Expr']].head())

# ====================== 11. STRING/TEXT OPERATIONS ======================
"""
STR ACCESSOR: .str.lower(), .str.contains(), .str.replace().
USEFUL: Gene name cleaning (e.g., remove prefixes), filter by motif.
"""
print("\n11. STRING OPS:")
df['Gene_Upper'] = df['Gene'].str.upper()
df['Has_MYC'] = df['Gene'].str.contains('MYC', case=False)
print("String ops (first 5):\n", df[['Gene', 'Gene_Upper', 'Has_MYC']].head())

# Replace
df['Sample_Clean'] = df['Sample'].str.replace('S', 'Sample_')
print("Replaced:\n", df['Sample_Clean'].head())

# ====================== 12. TIME SERIES (IF APPLICABLE) ======================
"""
TO_DATETIME: Convert to datetime.
RESAMPLE: Aggregate by time freq.
ROLLING: Moving windows.
USEFUL: Time-course RNA-Seq analysis.
"""
print("\n12. TIME SERIES:")
# Assume Time as hours, add date
df['Datetime'] = pd.to_datetime(df['Time'].astype(str) + 'H', format='%jH')  # Fake
print("Datetime (first 5):\n", df[['Time', 'Datetime']].head())

# Resample (group by Time)
ts_grouped = df.set_index('Datetime').resample('4H')['Expression'].mean()
print("Resampled mean:\n", ts_grouped)

# Rolling mean
df['Rolling_Mean'] = df.groupby('Gene')['Expression'].rolling(window=2, min_periods=1).mean().reset_index(0, drop=True)
print("Rolling mean (first 5):\n", df[['Gene', 'Expression', 'Rolling_Mean']].head())

# ====================== 13. ADVANCED: MULTIINDEX & WINDOW FUNCTIONS ======================
"""
MULTIINDEX: set_index with multiple cols.
EXPAND/EXPLODE: Nested lists.
USEFUL: Hierarchical gene-sample-time indexing.
"""
print("\n13. ADVANCED:")
# MultiIndex
multi_df = df.set_index(['Time', 'Gene'])
print("MultiIndex head:\n", multi_df.head())

# Swap levels
multi_df.swaplevel().head()

# Explode (if lists in cells)
df_explode = df.copy()
df_explode['Tags'] = [['tag1', 'tag2']] * len(df)  # Fake list
df_explode = df_explode.explode('Tags')
print("Exploded (first 3):\n", df_explode[['Gene', 'Tags']].head(3))

print("\n" + "=" * 80)
print("END OF CHEAT SHEET. Customize for specific assessment questions!")